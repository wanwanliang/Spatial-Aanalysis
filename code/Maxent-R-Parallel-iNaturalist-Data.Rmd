---
title: "Maxent Parallel SDM in R"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Get Distribution Data
```{r step1, include=F}

## Download species distribution data from iNaturalist or/and CABI.org

# get data from iNaturalist
library(rinat)
inat=get_inat_obs(query="Lobesia botrana")
inat2=inat[,c("longitude","latitude")]

# merge EGVM data downloaded from CABI
setwd("C:/Users/liang/Desktop/EGVM/.")
cabi=read.csv("LobesiaBotrana.csv")

cabi2=cabi[,c("Longitude","Latitude")]
colnames(cabi2)=colnames(inat2)
dtall=rbind(inat2,cabi2)
#write.csv(dtall,"Distribution_EGVM.csv",row.names = F)
```


## Delineate Region where PAs or Background Data should be Extracted 

```{r step2}
library(rgdal)
library(raster)

n=dim(dtall)[1]
occur=SpatialPointsDataFrame(dtall,data=data.frame(ID=1:n))
crs(occur)="+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"


bb=bbox(occur)
b_poly <- as(extent(as.vector(t(bb))), "SpatialPolygons")
b_poly=SpatialPolygonsDataFrame(b_poly,data=data.frame(ID=1))

#writeOGR(b_poly,".","EGVMPresenceBB",driver = "ESRI Shapefile")
```

## Construct SDM in the United States with MAXENT 
```{r step 3}

library(dismo)
library(geosphere)

## for occurrence data, remove points with distance <dis to remove data biase
dis_filter = function(pts,dist){
  
  n=dim(pts)[1]
  
  for (i in 1:n){
    if (i>n){break}
    
    dis=distm(pts,pts[i,])/1000 # convert m to km
    z=which(dis<dist & dis!=0)
    
    if (length(z)>0){
      pts=pts[-z,]
      n=n-length(z)
    }
  }
  return(pts)
}

occur2=dis_filter(occur,3)

setwd("C:/Users/liang/Desktop/EGVM/bioclim/wc2.0_2.5m_bio/")
vars=list.files(".",".tif$")
ra=raster(vars[1])
vars2=stack(vars)

## generate random points as background data for Maxent model within the given spatial region
bg_points=sampleRandom(raster(vars[1]),10000,sp=T,na.rm=T,ext=extent(b_poly),xy=T)

```

```{r plot1, echo=T }
plot(ra)
plot(b_poly,add=T)
plot(bg_points,add=T,col="blue",cex=0.5)
plot(occur,add=T,col="red",pch=4,cex=1.5)

```

### extract variable values for all Occurrence and Background Points
```{r step3}
preVar=extract(vars2,occur2)
bkVar=extract(vars2,bg_points)
preVar=as.data.frame(preVar)
bkVar=as.data.frame(bkVar)
head(preVar)

feas=rbind(preVar,bkVar)
resp=c(rep(1,dim(preVar)[1]),rep(0,dim(bkVar)[1]))

### load predictors for projection region, these predictors should have the same names with the ones for modeling construction
setwd("C:/Users/liang/Desktop/EGVM/bioclim/usAOI")
predictors=stack(list.files(".",".tif$")) 
```



## define a maxent function using prop data for training. For each run, the function return testing AUC and the trained maxent model

```{r step4}
maxentRun= function(feas,resp,prop,i){
  n=dim(feas)[1]
  n2=n*prop
  
  set.seed(6*i+66)
  id=sample(1:n,n2)
  trfeas=feas[id,]
  trresp=resp[id]
  tsfeas=feas[-id,]
  tsresp=resp[-id]
  
  model=maxent(x=trfeas,p=trresp)
  
  p=tsfeas[tsresp[]==1,]
  a=tsfeas[tsresp[]==0,]
  eva=evaluate(p,a,model)
  auc=eva@auc
  
  ls=list(model,auc)
  return(ls)
}

```
## Parallel multiple runs to speed up

```{r,results='asis'}
library(parallel)
library(foreach)
library(doParallel)

detectCores()
cl=makeCluster(10)
registerDoParallel(cl)

comb=function(auc,map){
  au=c(auc)
  mp=stack(map)
  return(list(au,mp))
}

multiRuns=list()

# only try 10 runs here, could be more
```

```{r, results='asis'}

multiRuns= foreach(i=1:10,  .multicombine = T,.export=c("maxent","evaluate"),.packages=c("dismo","rJava")) %dopar% {
  outs=maxentRun(feas,resp,0.8,i)
  auc=outs[[2]]
  map=predict(outs[[1]],predictors)
  
  list(auc,map)
}

multiRuns[[1]]
multiRuns[[2]]

auc=multiRuns[[1]][[1]]
map=stack(multiRuns[[1]][[2]])

for (i in 2:10){
  auc=c(auc,multiRuns[[i]][1])
  map=stack(map,multiRuns[[i]][[2]])
}

auc
map
```
